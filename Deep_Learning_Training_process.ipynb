{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVjgL8q4gwf0"
      },
      "source": [
        "## How to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42QfwaS2iiiW"
      },
      "source": [
        "Use this URL to access:\n",
        "\n",
        "**shorturl.at/gsIZ9**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-U92rzggwWU"
      },
      "source": [
        "To use this notebook first you need to create a copy in your own personnal google drive (as per the first picture of the first practical session). **Then you'll need to switch the runtime to GPU to be able to train your models on GPU to reduce runtime** (as per the second picture of the first practical session)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsAc4bdpt2WK"
      },
      "source": [
        "# Deep learning : **_Training_**\n",
        "\n",
        "After studying the preparation of data and the design of deep learning models, we focus in this session on the training process. We are going to see how to train a model and choose its training parameters, as well as visualize what models really learn and conclude by understanding transfer learning, a powerful technique to reuse previously trained models and apply them to other usecases.\n",
        "\n",
        "![Training overview](https://www.rocq.inria.fr/cluster-willow/tchabal/courses/springschool2022/overview_training.png)\n",
        "\n",
        "_Illustration created by Thomas Chabal and Cl√©ment Riu, 2022._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLmOI3oix3E-"
      },
      "source": [
        "## Setting up the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvNaNCxVx45D"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!wget https://www.rocq.inria.fr/cluster-willow/tchabal/courses/springschool2022/casablanca_course.zip && \\\n",
        "  unzip casablanca_course.zip && \\\n",
        "  rm casablanca_course.zip\n",
        "%cd casablanca_course\n",
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Udbj8Dk2aIG"
      },
      "source": [
        "## Choosing training settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNCvaUT92i9C"
      },
      "source": [
        "### Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNb01R0_3gZY"
      },
      "source": [
        "Deep learning is based on a principle of trial-and-error. A model makes a prediction from an input value and we compare this prediction with the expected value. That comparison leads to the computation of an error, which then helps to adjust the network's parameters.\n",
        "\n",
        "That loss value should be a differentiable function. Indeed, weights of the network are adjusted with respect to the loss derivative. The formula is simple, given weights of the model at step $t$ $w_t$ and a loss $\\mathcal{L}$, we update the weights as $w_{t+1} = w_t - \\frac{\\partial \\mathcal{L}}{\\partial w}$. This means that we have minimized the loss when we get $\\frac{\\partial \\mathcal{L}}{\\partial w} = 0$, i.e. the weights are not updated anymore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbpuFTLxnVqy"
      },
      "source": [
        "This loss function must be chosen according to the problem, to reach the goal we want. Some losses are more adapted to some problems. For instance,\n",
        "- When classifying images in several categories, it is common to use the [_Cross Entropy_ loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss).\n",
        "- When doing binary classification (i.e. classification with only 2 classes), we may use the [_Binary Cross Entropy_ loss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss):\n",
        "$$\\mathcal{L}(x, y) = y \\log(x) + (1-y) \\log(1-x)$$\n",
        "- When regressing values, the [_L2_](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) and [_L1_](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss) (or its [_Smooth L1_](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss) variant) losses can be adapted.\n",
        "There exists many other losses detailed [here](https://pytorch.org/docs/stable/nn.html#loss-functions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc6Kdg75nWWz"
      },
      "source": [
        "In Pytorch, we simply define the loss function as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5WRbRaSnbyF"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5IRdp6gnnn7"
      },
      "source": [
        "We then call this loss function during training with a code similar to: `loss = criterion(outputs, labels)`. All the computation is then performed by torch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJUa5XGX2kZ4"
      },
      "source": [
        "### Learning rate and optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbv4QN5foKDG"
      },
      "source": [
        "We explained that we update weights with a formula like $w := w - \\frac{\\partial \\mathcal{L}}{\\partial w}$. This is quite not accurate.\n",
        "\n",
        "In that formula, the derivative of the loss indicates a direction in the space of weights to reach the minimum of the loss function. This is the opposite direction of the loss function's gradient, which points towards the minimum of that function.\n",
        "\n",
        "Yet, we should also consider the size of the step to take: taking a too large step in the direction of the gradient could have us miss the minimum and get to a worse point.\n",
        "\n",
        "Choosing that gradient step size is done with what we call the _learning rate_. We usually note it $\\lambda$, and update the weights of the model as:\n",
        "$$w := w - \\lambda \\frac{\\partial \\mathcal{L}}{\\partial w}$$\n",
        "\n",
        "This learning rate is found after learning with different values and keeping the one which leads to the best performances. There is no rule of thumb to decide which learning rate to take. In practice, research papers publishing new models also present the learning rate they take and we simply reuse their value.\n",
        "\n",
        "There exists several formulas to update the weights of a model. We presented here the principle of the _Gradient Descent_, but there exists others like the [_Stochastic Gradient Descent_](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) or [_Adam_](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam). In torch, they are called [_optimizers_](https://pytorch.org/docs/stable/optim.html#algorithms)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsSwRimSqqTM"
      },
      "source": [
        "Pytorch handles the optimization step to update the weights of the model. All we need to do is to define an _optimizer_ with its parameters, as in the following cell. Optimizers list can be found [here](https://pytorch.org/docs/stable/optim.html#algorithms)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALqWa0SOruMs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a neural network\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "\n",
        "# Define the optimizer with the network's weights (or parameters), the learning rate and possibly other parameters\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMqGb_UFsg5b"
      },
      "source": [
        "### Learning rate scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7yOJuAtsi6L"
      },
      "source": [
        "During the training, we may decide to reduce the learning rate. This is appropriate when the loss doesn't evolve much after a while as the network's weights get close to a minimum of the loss. Reducing the learning rate then allows the network to do minor and more refined updates of weights and therefore get closer to that minimum.\n",
        "\n",
        "This update of the learning rate is done with what is called a [_Learning rate scheduler_](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate).\n",
        "\n",
        "This is again easy to define and use in a training pipeline, but we won't get more into details to focus on the other notions presented above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC1eJk6Q2pye"
      },
      "source": [
        "## Writing a training script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb22-5znwAt0"
      },
      "source": [
        "### The training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8vr0r73tmIF"
      },
      "source": [
        "Let's get now to the training pipeline !\n",
        "\n",
        "**_Reminder_**: To train a model, we need:\n",
        "- A _model_;\n",
        "- Data to train on, i.e. a _train dataloader_;\n",
        "- A _loss function_, or _criterion_;\n",
        "- An _optimizer_ to update the model's weights.\n",
        "\n",
        "The **training process** is then very simple:\n",
        "- We **take a batch of data from the dataloader**, this batch being made of inputs and labels;\n",
        "- The **model predicts outputs** from the inputs;\n",
        "- We **compute a loss** between the predictions and the labels;\n",
        "- We **update the model** given that loss.\n",
        "\n",
        "The next cell defines a training function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4NFaYjEtkgg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_loop(net, trainloader, criterion, optimizer):\n",
        "    running_loss = 0.0\n",
        "    with tqdm(trainloader, desc=\"Training\") as pbar:\n",
        "        for (inputs, labels) in pbar:\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            # .cuda() sends the data to the GPU\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            pbar.set_postfix({\"train_loss\": f\"{running_loss:.3f}\"})\n",
        "            running_loss = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csdHxzHBvVNF"
      },
      "source": [
        "The previous code makes one pass over the whole train dataloader. We can already test it on some data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Izx4tdMfveDX"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "from casablanca_course.training import load_dataset, build_net\n",
        "\n",
        "dataset = \"cifar10\"\n",
        "\n",
        "trainloader, testloader = load_dataset(dataset, batch_size=32, train_share=50)\n",
        "\n",
        "net = build_net(dataset).cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7xnFDgXxwD9"
      },
      "outputs": [],
      "source": [
        "train_loop(net, trainloader, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEj4cL7swDUp"
      },
      "source": [
        "### The validation loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiSj584PyuRS"
      },
      "source": [
        "We may evaluate the quality of the model during the training. To do so, we previously defined a _validation dataset_. The evaluation consists simply in making predictions for each of the inputs and comparing them with the labels. No update of the model is done during this step, so it is also useless to compute the gradient of the loss.\n",
        "\n",
        "The following cell defines an evaluation of the model, given a neural network, a validation dataloader and a loss function (or criterion):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z53i9qqwzOZg"
      },
      "outputs": [],
      "source": [
        "def test_loop(net, testloader, criterion):\n",
        "    # Tell torch to not compute gradients during predictions. This saves significant computational time.\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        with tqdm(testloader, desc=\"Validation\") as pbar:\n",
        "            for (inputs, labels) in pbar:\n",
        "                # Extract inputs and labels from the dataloader, and send them to the GPU\n",
        "                inputs = inputs.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "                # forward pass and loss computation, with no backward or optim\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # print statistics\n",
        "                val_loss += loss.item()\n",
        "                pbar.set_postfix(\n",
        "                    {\"val_loss\": f\"{loss.item() :.3f}\"})\n",
        "        print(\n",
        "            f'Validation loss: {val_loss / len(testloader):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jd_SLThzk6T"
      },
      "outputs": [],
      "source": [
        "test_loop(net, testloader, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUNLt6wezrJQ"
      },
      "source": [
        "During training, both the training and the validation losses should keep decreasing.\n",
        "\n",
        "The **training shall be stopped when the validation loss starts increasing again while the training loss decreases**. The regime in which the training loss decreases while the validation loss increases is called _overfitting_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynLgsJJV0C-D"
      },
      "source": [
        "### Epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6LCUIVP0MtK"
      },
      "source": [
        "During training, we make several passes over the training dataset. Each of these passes is called an **epoch**: during one epoch, each image is processed exactly once by the model.\n",
        "\n",
        "The epochs are usually an alternance of a training step, going over the whole training set, and an evaluation step, evaluating the performance of the model on the validation set as a control of the training.\n",
        "\n",
        "The following cell presents the whole training process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej2SLyS4vAFD"
      },
      "outputs": [],
      "source": [
        "def training_process(net, trainloader, testloader, criterion, optimizer, n_epochs):\n",
        "    # For each epoch, i.e. going over the training set once\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"=\" * 20, f\"\\nEpoch {epoch + 1}\")\n",
        "\n",
        "        # Training phase\n",
        "        train_loop(net, trainloader, criterion, optimizer)\n",
        "\n",
        "        # Evaluation phase\n",
        "        test_loop(net, testloader, criterion)\n",
        "\n",
        "    print('\\n\\nFinished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAoN0lMB09EJ"
      },
      "outputs": [],
      "source": [
        "training_process(net, trainloader, testloader, criterion, optimizer, n_epochs=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPdgsQrX1C5h"
      },
      "source": [
        "We used only 4 epochs here to have a run the cell fast, but training completely the network would require hundreds or thousands of epochs, which would represent hours or days of computation on a GPU.\n",
        "\n",
        "We printed losses but we can also store them during training and plot them as curves of the loss as a function of epochs or seen batches. A useful tool for that is [Tensorboard](https://pytorch.org/docs/stable/tensorboard.html), which you may have a look at in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbOzVPC5wFJu"
      },
      "source": [
        "### The whole code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bckbACKw2Yzu"
      },
      "source": [
        "To get a general overview of a deep learning code, we summarize in the next cell all the code we wrote in the previous sessions in order to train a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtAPvmlc2lNW"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "\n",
        "# Define the train and validation dataloaders\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = CIFAR10(root='./data', train=True, download=True, transform=train_transforms)\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = CIFAR10(root='./data', train=False, download=True, transform=test_transforms)\n",
        "testloader = DataLoader(testset, batch_size=1024, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"The training set is composed of {len(trainset)} images, while the test set includes {len(testset)} images.\")\n",
        "\n",
        "\n",
        "# Define the model\n",
        "net = nn.Sequential(\n",
        "    nn.Conv2d(3, 6, 5),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "\n",
        "    nn.Conv2d(6, 16, 5),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "\n",
        "    nn.Flatten(),\n",
        "\n",
        "    nn.Linear(16 * 5 * 5, 120),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(120, 84),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(84, 10)\n",
        ")\n",
        "net = net.cuda()\n",
        "\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "\n",
        "def training_process(net, trainloader, testloader, criterion, optimizer, n_epochs):\n",
        "    # For each epoch, i.e. going over the training set once\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"=\" * 20, f\"\\nEpoch {epoch + 1}\")\n",
        "\n",
        "        # Training phase\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        with tqdm(trainloader, desc=\"Training\") as pbar:\n",
        "            for (inputs, labels) in pbar:\n",
        "                # get the inputs; data is a list of [inputs, labels]\n",
        "                inputs = inputs.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # print statistics\n",
        "                running_loss += loss.item()\n",
        "                pbar.set_postfix({\"train_loss\": f\"{running_loss:.3f}\"})\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Evaluation phase\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "          val_loss = 0.0\n",
        "          with tqdm(testloader, desc=\"Validation\") as pbar:\n",
        "              for (inputs, labels) in pbar:\n",
        "                  # Extract inputs and labels from the dataloader, and send them to the GPU\n",
        "                  inputs = inputs.cuda()\n",
        "                  labels = labels.cuda()\n",
        "\n",
        "                  # forward pass and loss computation, with no backward or optim\n",
        "                  outputs = net(inputs)\n",
        "                  loss = criterion(outputs, labels)\n",
        "\n",
        "                  # print statistics\n",
        "                  val_loss += loss.item()\n",
        "                  pbar.set_postfix(\n",
        "                      {\"val_loss\": f\"{loss.item() :.3f}\"})\n",
        "          print(\n",
        "              f'Validation loss: {val_loss / len(testloader):.3f}')\n",
        "\n",
        "    print('\\n\\nFinished Training')\n",
        "\n",
        "training_process(net, trainloader, testloader, criterion, optimizer, n_epochs=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0_-kXJ33PKr"
      },
      "source": [
        "## Visualize CNN filters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1xE6ZUya0Ao"
      },
      "source": [
        "### Visualizing features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D536k_TxUF2D"
      },
      "source": [
        "To better understand how neural networks, and in particular convolutional ones, work, it is worth visualizing their filters.\n",
        "\n",
        "When feeding an image to a convolutional neural network, this image is processed by a succession of convolutional layers and _activation_ layers, i.e. non-linear functions. To understand the role of a given convolutional filter on the output, we can have a look at how the output of that layer looks like given some input image.\n",
        "\n",
        "Indeed, as we saw in the first session, a CNN filter of kernel $K$ acts on an image $I$ as:\n",
        "\n",
        "$$(I * K)[x, y] = b + \\sum_{i,j=-k}^k I[x+i, y+j] K[i, j]$$\n",
        "\n",
        "Once the network has been trained, we don't change its weights anymore. Therefore $K$ is constant, and the output value $I * K$ of that filter only depends on the input image.\n",
        "\n",
        "What we can do now is to look for the image $I$ that leads to the largest value of $I*K$ on average, i.e. the image which leads to the largest _activation_ of the filter. We solve the problem:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{array}{rrclcl}\n",
        "\\displaystyle \\max_{I} \\frac{1}{WH}\\sum_{x, y \\in I} (I * K)[x,y] =\n",
        "\\displaystyle \\min_{I} - \\frac{1}{WH}\\sum_{x, y \\in I} (I * K)[x,y]\n",
        "\\end{array}\n",
        "\\end{equation}\n",
        "\n",
        "The optimum is an image which we visualize. We do this in the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-AAptL1t09c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Adapted from https://github.com/utkuozbulak/pytorch-cnn-visualizations/blob/master/src/cnn_layer_visualization.py\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torchvision import models\n",
        "from casablanca_course.training.layer_viz_utils import preprocess_image, recreate_image\n",
        "\n",
        "\n",
        "class CNNLayerVisualization():\n",
        "    \"\"\"\n",
        "        Produces an image that minimizes the loss of a convolution\n",
        "        operation for a specific layer and filter\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, selected_layer, selected_filter, n_optim_iters=30):\n",
        "        self.model = model\n",
        "        self.model.eval().cuda()\n",
        "        self.selected_layer = selected_layer\n",
        "        self.selected_filter = selected_filter\n",
        "        self.n_optim_iters = n_optim_iters\n",
        "\n",
        "    def create_random_image(self):\n",
        "        # Generate a random image\n",
        "        random_image = np.uint8(np.random.uniform(150, 180, (224, 224, 3)))\n",
        "        return preprocess_image(random_image)\n",
        "\n",
        "    def visualise_layer(self, visualize=True):\n",
        "        # Generate a random image\n",
        "        processed_image = self.create_random_image()\n",
        "\n",
        "        # Define optimizer for the image\n",
        "        optimizer = Adam([processed_image], lr=0.1, weight_decay=1e-6)\n",
        "\n",
        "        for _ in range(self.n_optim_iters):\n",
        "            optimizer.zero_grad()\n",
        "            # Assign create image to a variable to move forward in the model\n",
        "            x = processed_image.cuda()\n",
        "\n",
        "            for index, layer in enumerate(self.model):\n",
        "                # Forward pass layer by layer\n",
        "                x = layer(x)\n",
        "                if index == self.selected_layer:\n",
        "                    # Stop at the selected layer\n",
        "                    break\n",
        "\n",
        "            # Select the filter we are interested in, compute the loss and optimize the input image\n",
        "            conv_output = x[0, self.selected_filter]\n",
        "            # Loss function is the mean of the output of the selected layer/filter\n",
        "            # We try to minimize the mean of the output of that specific filter\n",
        "            loss = -torch.mean(conv_output)\n",
        "            # Backward\n",
        "            loss.backward()\n",
        "            # Update image\n",
        "            optimizer.step()\n",
        "\n",
        "        # Recreate image\n",
        "        self.recreated_image = recreate_image(processed_image)\n",
        "        if visualize:\n",
        "            plt.imshow(self.recreated_image)\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj3_X_qoaf1U"
      },
      "outputs": [],
      "source": [
        "# Take a pretrained model\n",
        "pretrained_model = models.vgg16(pretrained=True).features\n",
        "\n",
        "# We pick a filter of a given layer and visualize it\n",
        "cnn_layer = 2\n",
        "filter_pos = 4\n",
        "\n",
        "# Layer visualization\n",
        "layer_vis = CNNLayerVisualization(pretrained_model, cnn_layer, filter_pos)\n",
        "layer_vis.visualise_layer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnjvYXuNFr1_"
      },
      "outputs": [],
      "source": [
        "# We pick a filter of a given layer and visualize it\n",
        "cnn_layer = 24\n",
        "filter_pos = 1\n",
        "\n",
        "# Layer visualization\n",
        "layer_vis = CNNLayerVisualization(pretrained_model, cnn_layer, filter_pos)\n",
        "layer_vis.visualise_layer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30XlM2TVarGS"
      },
      "source": [
        "Change the layers and filters in the previous cells to visualize filters at various depths in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_YmFUTfa34H"
      },
      "source": [
        "### Comparing trained and not trained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fro_HIbcmRW"
      },
      "source": [
        "We compare in what follows these features between a trained version and an untrained one of the same architecture.\n",
        "\n",
        "Once again, you may change the layers to visualize different filters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDDqlCrlYdOd"
      },
      "outputs": [],
      "source": [
        "def compare_before_after_training(cnn_layer, filter_pos):\n",
        "    # Select trained and not trained models\n",
        "    untrained_model = models.vgg16(pretrained=False).features\n",
        "    trained_model = models.vgg16(pretrained=True).features\n",
        "\n",
        "    untrained_layer_vis = CNNLayerVisualization(untrained_model, cnn_layer, filter_pos)\n",
        "    trained_layer_vis = CNNLayerVisualization(trained_model, cnn_layer, filter_pos)\n",
        "\n",
        "    untrained_layer_vis.visualise_layer(visualize=False)\n",
        "    trained_layer_vis.visualise_layer(visualize=False)\n",
        "\n",
        "    # Display recreated images\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    plt.subplot(121)\n",
        "    plt.imshow(untrained_layer_vis.recreated_image)\n",
        "    plt.title(\"Untrained model feature\")\n",
        "    plt.subplot(122)\n",
        "    plt.imshow(trained_layer_vis.recreated_image)\n",
        "    plt.title(\"Trained model feature\")\n",
        "\n",
        "# We pick a filter of a given layer and visualize it\n",
        "cnn_layer = 24\n",
        "filter_pos = 3\n",
        "\n",
        "compare_before_after_training(cnn_layer, filter_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWA2lV9VdEBK"
      },
      "source": [
        "With these visualizations, we notice significant differences between images that activate both models:\n",
        "- On the one hand, the untrained network is activated by a white noise. It means its filters do not detect any structure in the inputs.\n",
        "- On the other hand, the trained network captures more and more complex structures as we get deep in the model.\n",
        "\n",
        "These complex structures is of particular interest: they have been obtained with training on massive data and they represent very important information in images that help to classify them.\n",
        "\n",
        "While these models are trained on data that is irrelevant to most industrial applications, we would still like to reuse these features and models for our own problems. This is possible with what is called _transfer learning_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fosoBH5UT8LT"
      },
      "source": [
        "## Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGP32tUiJy_Q"
      },
      "source": [
        "### Saving and loading a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLGfiOZ-J1ai"
      },
      "source": [
        "Training a model takes a very long time and is computation intensive. Therefore, we do not train a model everytime we run a program: we rather save trained networks and load them when it is necessary.\n",
        "\n",
        "Both operations can be done in a single line of code.\n",
        "\n",
        "During training, we may save the model whenever we find it relevant: we can choose to save the model at the end of each epoch or when we improve the validation accuracy for instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDqZFeeBMa-r"
      },
      "source": [
        "First, let us create a small neural network and train it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHFVR8jBLjFN"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from casablanca_course.models import evaluate_net, train_for_net\n",
        "\n",
        "def create_cnn():\n",
        "  return nn.Sequential(\n",
        "      nn.Conv2d(1, 6, 3),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(6, 3, 3),\n",
        "      nn.ReLU(),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(24 * 24 * 3, 256),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(256, 10),\n",
        "      nn.Softmax(dim=1),\n",
        "  )\n",
        "\n",
        "my_cnn = create_cnn()\n",
        "\n",
        "def compare_before_after(net, batch_size=128, n_epochs=4, lr=0.001):\n",
        "  print(\"Evaluation before training the network\")\n",
        "  net.eval()\n",
        "  evaluate_net(net, dataset=\"mnist\")\n",
        "\n",
        "  print(\"\\n\\n\")\n",
        "  net.train()\n",
        "  train_for_net(net, dataset=\"mnist\", batch_size=128, n_epochs=4, lr=lr)\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  print(\"Evaluation after training the network\")\n",
        "  net.eval()\n",
        "  evaluate_net(net, dataset=\"mnist\")\n",
        "\n",
        "\n",
        "compare_before_after(my_cnn, lr=3e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ5E1WgxTc1D"
      },
      "source": [
        "We have trained our model, and we can now have a look at its weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAE2ylk8TcWt"
      },
      "outputs": [],
      "source": [
        "my_cnn.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TBKfKpfTuxo"
      },
      "source": [
        "As we saw previously, our model is made of weights representing the convolutional kernels for the Conv2d layers, and weight matrices for the linear layers, as well as bias vectors.\n",
        "\n",
        "We now want to save the model in order to reuse it later. Here is how we do it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nugbn9uBTvfd"
      },
      "outputs": [],
      "source": [
        "torch.save(my_cnn.state_dict(), \"./my_model_weights.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCe6V11kUWkl"
      },
      "outputs": [],
      "source": [
        "!ls -l my_model_weights.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk3aKK0zVsCL"
      },
      "source": [
        "In our case, our model weighs around 1.8Mb.\n",
        "\n",
        "Now, assume our model is not in memory anymore. We would like to load it again to use it. To do so, all we have to do is to create an instance of the model and load its weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkQcNWPJWNe8"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the model\n",
        "loaded_model = create_cnn()\n",
        "\n",
        "loaded_model.eval()\n",
        "evaluate_net(loaded_model, dataset=\"mnist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRpXWpe_WZ3o"
      },
      "source": [
        "Our model performs very bad with a low accuracy. Indeed, its weights have only been randomly initialized.\n",
        "\n",
        "We load it with the following line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1tYXxE1WnD3"
      },
      "outputs": [],
      "source": [
        "# Load the weights stored in the previously saved file\n",
        "loaded_model.load_state_dict(torch.load(\"./my_model_weights.pth\"))\n",
        "\n",
        "loaded_model.eval()\n",
        "evaluate_net(loaded_model, dataset=\"mnist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI3U3REiW2Zn"
      },
      "source": [
        "Indeed, our model now performs as well as after the previous training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxhamTHA8fNN"
      },
      "source": [
        "### Training from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ978d6U7eds"
      },
      "source": [
        "Let us get back to the CIFAR 10 dataset. We would like to train a VGG16 model on this dataset.\n",
        "\n",
        "We can simply take a VGG model available online and run it on the testset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynv3zwpO2S1b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from tqdm import tqdm\n",
        "from casablanca_course.training.eval import evaluate_net\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "testset = CIFAR10(root='./data', train=False, download=True, transform=test_transforms)\n",
        "testloader = DataLoader(testset, batch_size=1024, shuffle=False, num_workers=2)\n",
        "\n",
        "num_classes = len(testloader.dataset.classes)\n",
        "print(f\"Data contains {num_classes}¬†classes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qukNxbdE4eQU"
      },
      "outputs": [],
      "source": [
        "net = torchvision.models.vgg16(pretrained=False).cuda()\n",
        "evaluate_net(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6qSkBfx7vrd"
      },
      "source": [
        "The accuracy with a randomly initialized and untrained network is very poor. The same occurs with a pretrained network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK3jGYaD4hp-"
      },
      "outputs": [],
      "source": [
        "net = torchvision.models.vgg16(pretrained=True).cuda()\n",
        "evaluate_net(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQPRjQ7O7E9j"
      },
      "source": [
        "Indeed, the current model is built to classify images into 1000 classes whereas our data is split in only 10 classes. The next cell shows the architecture of our network, whose final layer (classifier.6) has an output size of 1000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6GFexo37OiT"
      },
      "outputs": [],
      "source": [
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WilJ38o7-OA"
      },
      "source": [
        "Instead, we can build a model which classifies images in only 10 categories. We do it by only changing the output size of the last layer of the VGG model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax2GI6GByv1J"
      },
      "outputs": [],
      "source": [
        "net = torchvision.models.vgg16(pretrained=False, num_classes=10).cuda()\n",
        "print(net)\n",
        "evaluate_net(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McBYj6MC8HuS"
      },
      "source": [
        "This time, we get an accuracy around 10%, which corresponds to random classification among 10 categories.\n",
        "\n",
        "We can train the model from scratch as we did before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jfxeangT8sE"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = CIFAR10(root='./data', train=True, download=True, transform=train_transforms)\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "\n",
        "training_process(net, trainloader, testloader, criterion, optimizer, n_epochs=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVotLzFu9FTX"
      },
      "outputs": [],
      "source": [
        "evaluate_net(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-canI5Hj8nJc"
      },
      "source": [
        "We notice that this training takes a long time. Our dataset is small and made of very small images, but the training may already take at least a few dozen minutes before converging.\n",
        "\n",
        "In industry, most datasets are made of large images, which makes the training more tedious and much longer.\n",
        "\n",
        "This costs time but also computational resources and hence money."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m283zSVN82nC"
      },
      "source": [
        "### Reusing features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfcyrVL2-qkn"
      },
      "source": [
        "We can save much time by reusing a previously trained model. Here, we adapt a VGG model trained on massive data in order to make it efficient on our own problem.\n",
        "\n",
        "We start by reusing a trained VGG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPOU6slb81y8"
      },
      "outputs": [],
      "source": [
        "net = torchvision.models.vgg16(pretrained=True)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUMXu9E3_em-"
      },
      "source": [
        "This model is fully trained but classifies data into 1000 categories. We need to classify images in only 10 categories while keeping the same features.\n",
        "\n",
        "First, we _freeze_ the weights, i.e. we prevent them from being updated and keep their value constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIoj_hv1AjcN"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "  total_params = sum(p.numel() for p in model.parameters())\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  return total_params, trainable_params\n",
        "\n",
        "total, trainable = count_parameters(net)\n",
        "print(f\"Before freezing, the model has {total} parameters, among which {trainable} are trainable.\")\n",
        "\n",
        "for param in net.parameters():\n",
        "    # Remove possibility to update each parameter/weight\n",
        "    param.requires_grad = False\n",
        "\n",
        "total, trainable = count_parameters(net)\n",
        "print(f\"After freezing, the model has {total} parameters, among which {trainable} are trainable.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzutnaJ4B_zE"
      },
      "source": [
        "Now that the weights are frozen, we can replace the last trainable layer to fit our problem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aREbXckhCFQg"
      },
      "outputs": [],
      "source": [
        "in_features = net.classifier[6].in_features\n",
        "net.classifier[6] = nn.Linear(in_features, num_classes)\n",
        "\n",
        "print(net)\n",
        "total, trainable = count_parameters(net)\n",
        "print(f\"After changing the last layer, the model has {total} parameters, among which {trainable} are trainable.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VNWwjRDC-Nm"
      },
      "source": [
        "We now have a VGG model that outputs 10 values instead of 1000. The point of interest lies in the number of parameters to train: while training from scratch requires to tune around 134 million parameters, we have reduced that number to barely 40 000. This is much less demanding in terms of data and training time.\n",
        "\n",
        "We can evaluate the accuracy of that changed model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scWoQJSqDSa8"
      },
      "outputs": [],
      "source": [
        "net = net.cuda()\n",
        "evaluate_net(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uELILPfDZdZ"
      },
      "source": [
        "The accuracy still is around 10% as we replaced the last layer by a randomly initialized one.\n",
        "\n",
        "Yet, we can train a bit the model as we did before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rAKSpT9B0l-"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.SGD(net.classifier.parameters(), lr=1e-4, momentum=0.9)\n",
        "\n",
        "training_process(net, trainloader, testloader, criterion, optimizer, n_epochs=2)\n",
        "\n",
        "evaluate_net(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2TZT8AeGmXb"
      },
      "source": [
        "With only 2 training steps with the pre-trained model, we perform better than the model trained from scratch during 4 epochs.\n",
        "\n",
        "If the trainings should be run for longer to truly show this phenomenon, _finetuning_ deep learning models as we did saves time and computation, which is very useful in numerous applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsdgn4uhChhW"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRR5S_W1Cirz"
      },
      "source": [
        "- [Most helpful forum for finding a solution to fix errors you may get while developing](https://stackoverflow.com)\n",
        "- [Loss functions in Pytorch](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "- [Github repository for visualizing CNN features](https://github.com/utkuozbulak/pytorch-cnn-visualizations)\n",
        "- [Some common loss functions](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n",
        "- [Wikipedia explanation of transfer learning](https://en.wikipedia.org/wiki/Transfer_learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ER-qYx4k_AM"
      },
      "source": [
        "_Practical session written by Thomas Chabal and Cl√©ment Riu ‚Ä¢ Spring School on Data Science - Ecole Centrale Casablanca ‚Ä¢ 2022._"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
