{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF2G5g8CfnO5"
      },
      "source": [
        "## How to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ekexTi8iS9C"
      },
      "source": [
        "Use this URL to access:\n",
        "\n",
        "**shorturl.at/bevwE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJIF7R3CfqUm"
      },
      "source": [
        "To use this notebook first you need to create a copy in your own personnal google drive (as per the first picture of the first practical session). **Then you'll need to switch the runtime to GPU to be able to train your models on GPU to reduce runtime** (as per the second picture of the first practical session)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaPeRoBaskSW"
      },
      "source": [
        "# Deep learning : **_Data_**\n",
        "\n",
        "In this session, we overview the preparation of data to be fed to a neural network.\n",
        "\n",
        "![Data overview](https://www.rocq.inria.fr/cluster-willow/tchabal/courses/springschool2022/overview_data.png)\n",
        "\n",
        "_Illustration created by Thomas Chabal and ClÃ©ment Riu, 2022._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV0voARA6oi4"
      },
      "source": [
        "## Setting up the notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpOe6oFZ6qqj"
      },
      "source": [
        "We first download code that will be useful for this session.\n",
        "\n",
        "As we are going to use a GPU, do not forget to activate a GPU for this notebook. To do so, go in _Execution_ > _Change the execution type_ > select _GPU_ in the _Hardware accelerator_ drop-down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxKt1m4TPOR3"
      },
      "outputs": [],
      "source": [
        "%cd /contentse.zip\n",
        "%cd casablanca_course\n",
        "!python setup.py in\n",
        "!wget https://www.rocq.inria.fr/cluster-willow/tchabal/courses/springschool2022/casablanca_course.zip && \\\n",
        "  unzip casablanca_course.zip && \\\n",
        "  rm casablanca_course.zip\n",
        "%cd casablanca_course\n",
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWx1MHBQsu2h"
      },
      "source": [
        "## Create a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_pko5u27BFd"
      },
      "source": [
        "### Download images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4kLskIaseSc"
      },
      "outputs": [],
      "source": [
        "# Download data and uncompress it\n",
        "\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
        "!mkdir -p samples\n",
        "!gunzip -c t10k-labels-idx1-ubyte.gz > samples/t10k-labels-idx1-ubyte\n",
        "!gunzip -c t10k-images-idx3-ubyte.gz > samples/t10k-images-idx3-ubyte\n",
        "\n",
        "!pip install python-mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgBULwS4yIVY"
      },
      "outputs": [],
      "source": [
        "from mnist import MNIST\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "data_dir = \"custom_dataset\"\n",
        "NB_IMAGES = 1000\n",
        "\n",
        "\n",
        "def uncompress_ds():\n",
        "  mndata = MNIST('samples')\n",
        "  images, labels = mndata.load_testing()\n",
        "\n",
        "  p = Path(data_dir)\n",
        "\n",
        "  images_subset = images[:NB_IMAGES]\n",
        "  labels_subset = labels[:NB_IMAGES]\n",
        "\n",
        "  LABELS = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "  LABELS_DIR = { idx: label.replace(\" \", \"-\").replace(\"/\", \"--\") for idx, label in enumerate(LABELS) }\n",
        "  for _, lbl in LABELS_DIR.items():\n",
        "    (p / lbl).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  def convert_to_img(image, label, idx):\n",
        "    img = np.array(image).reshape((28, 28)).astype(np.uint8)\n",
        "    lbl = LABELS_DIR[label]\n",
        "    file_name = p / lbl / f\"{lbl}_{idx}.png\"\n",
        "    Image.fromarray(img).save(file_name, \"PNG\")\n",
        "\n",
        "  _ = [convert_to_img(img, label, idx) for idx, (img, label) in enumerate(zip(images_subset, labels_subset))]\n",
        "\n",
        "uncompress_ds()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7bJFmw07FDj"
      },
      "source": [
        "### Visualize a few images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQG6nVcg8jSz"
      },
      "source": [
        "Our images are stored in the `custom_dataset` directory. These are images of various clothes from categories that we list below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5z6A_WkM47kg"
      },
      "outputs": [],
      "source": [
        "!ls custom_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca-Nm5c08zLD"
      },
      "source": [
        "When developing a deep learning application, knowing the data is crucial. It must direct you in your choices of model architecture, training parameters, data augmentation...\n",
        "\n",
        "We visualize a few images in what follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Azl7ZOp-0OP8"
      },
      "outputs": [],
      "source": [
        "class_dirs = [p for p in Path(data_dir).glob(\"*\")]\n",
        "nb_classes = len(class_dirs)\n",
        "\n",
        "plt.figure(figsize=(30,30))\n",
        "nb_cols = 5\n",
        "nb_rows = int(np.ceil(nb_classes / nb_cols))\n",
        "\n",
        "plt.subplots(nb_rows, nb_cols)\n",
        "for idx, category in enumerate(class_dirs):\n",
        "  plt.subplot(nb_rows, nb_cols, idx + 1)\n",
        "  imgs = [p for p in category.glob(\"*\")]\n",
        "  img_path = np.random.choice(imgs)\n",
        "  img = np.array(Image.open(img_path))\n",
        "  plt.imshow(img, cmap=\"gray\")\n",
        "  plt.axis(\"off\")\n",
        "  label = category.stem\n",
        "  plt.title(label)\n",
        "  if idx == 0:\n",
        "    print(f\"Images are of shape {img.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0dKg4Xa5TYw"
      },
      "source": [
        "As shown above, our images are small images (of size 28x28) and have only one color channel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhZZHkh95w8M"
      },
      "source": [
        "### From directories of images to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkRffYyG55rn"
      },
      "source": [
        "Neural networks process several images at once. When trained in a \"supervised\" way, they compute predictions from the input images and compare them to labels.\n",
        "\n",
        "We must therefore create a structure adapted to these massive computations: a **dataset**.\n",
        "\n",
        "In Pytorch, datasets have a simple structure. They are implemented as classes with 3 functions:\n",
        "- `__init__` initializes the class;\n",
        "- `__len__` can be used to have an idea of how big the dataset is, i.e. of how many images it is composed;\n",
        "- `__getitem__` prepares one input, and its label in the case of supervised learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJt8MLwt604s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, img_paths, categories, transform=None):\n",
        "        self.img_paths = img_paths\n",
        "        self.categories = categories\n",
        "        self.labels = { category: idx for idx, category in enumerate(self.categories) }\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        image = read_image(str(img_path))\n",
        "        obj_category = img_path.parent.name\n",
        "        label = self.labels[obj_category]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WiQCP5MC0Ac"
      },
      "source": [
        "Here we defined that we would give as input to the dataset class the set of paths to our images as well as the categories of objects. Note that the structure of the image directories can be different, in which case we would adapt the image paths and categories to fit our structure.\n",
        "\n",
        "Let's now get this data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcQWCMBe88AG"
      },
      "outputs": [],
      "source": [
        "img_dir = Path(data_dir)\n",
        "img_paths = [p for p in img_dir.rglob(\"*\") if not p.is_dir()]\n",
        "categories = [p.name for p in img_dir.glob(\"*\") if p.is_dir()]\n",
        "\n",
        "custom_dataset = CustomImageDataset(img_paths, categories)\n",
        "print(f\"The dataset is made of {len(custom_dataset)} samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO3kjsJCCiPN"
      },
      "outputs": [],
      "source": [
        "idx = 42\n",
        "sample = custom_dataset[idx]\n",
        "img, label = sample\n",
        "print(f\"A sample is composed of an image of type {type(img)} and shape {img.shape}, and of a label (here: {label})\")\n",
        "\n",
        "plt.imshow(img[0, :, :], cmap=\"gray\")\n",
        "plt.title(f\"Label: {label}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zotPUFbDfYj"
      },
      "source": [
        "The creation of a torch Dataset with thousands of images is very fast. Indeed, images are not loaded at the same time in memory. Instead, they are opened and processed only when we call the `__getitem__` function, for instance by calling `custom_dataset[0]`. During the training of a neural network, this loading may take time if the function is heavy to compute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5stuDGL_NQf"
      },
      "source": [
        "### Split in train, validation and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dERqfD9ZESd6"
      },
      "source": [
        "To train a model and evaluate properly its performance, we need to split our data in 3 sets:\n",
        "- The _training_ set gathers all the images that will be fed in the network and for which the network will predict values and learn,\n",
        "- The _validation_ set gathers another set of images for which the network only predicts values without modifying its weights. We measure the _accuracy_ of the model on this validation set, which then informs us on how well the model learns and generalizes to unseen images. This validation set guides us in the training and is the reference to define when we stop the training.\n",
        "- The _test_ set gathers other images. We also measure the accuracy on this test set, but it should not impact the training, i.e. no decision of stopping the training should be taken by looking at this set. Labels of these images are often unknown to avoid cheating, and the purpose of this test set is to have an impartial evaluation of the model.\n",
        "\n",
        "Let us separate our data in these sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1MP_WXVFh1T"
      },
      "outputs": [],
      "source": [
        "# Define the percentage of samples to attribute to the train, validation or test sets\n",
        "SHARES = {\n",
        "    \"train\": 70,\n",
        "    \"val\": 20,\n",
        "    \"test\": 10,\n",
        "}\n",
        "\n",
        "# Compute this split\n",
        "nb_train_imgs = int(SHARES[\"train\"] * len(custom_dataset) / 100)\n",
        "nb_val_imgs = int(SHARES[\"val\"] * len(custom_dataset) / 100)\n",
        "\n",
        "training_paths = img_paths[:nb_train_imgs]\n",
        "validation_paths = img_paths[nb_train_imgs:nb_train_imgs + nb_val_imgs]\n",
        "test_paths = img_paths[nb_train_imgs + nb_val_imgs:]\n",
        "\n",
        "training_dataset = CustomImageDataset(training_paths, categories)\n",
        "validation_dataset = CustomImageDataset(validation_paths, categories)\n",
        "test_dataset = CustomImageDataset(test_paths, categories)\n",
        "\n",
        "print(f\"Our sets are made of:\")\n",
        "print(f\"- {len(training_dataset)} images for the training set,\")\n",
        "print(f\"- {len(validation_dataset)} images for the validation set,\")\n",
        "print(f\"- {len(test_dataset)} images for the test set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9_YkP0F_LOv"
      },
      "source": [
        "### Create a dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWTT7uagD6R7"
      },
      "source": [
        "The dataset we defined allows us to load images one by one and always in the same order. Instead, during training, we randomize the order in which the network sees images, and we also process several images at once. To do so, we wrap our datasets in another torch structure: the `DataLoader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okYvm_0y-QPr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_dataset, batch_size=8, shuffle=True)\n",
        "val_dataloader = DataLoader(validation_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQMwQ8-FGxc5"
      },
      "source": [
        "We can take a look at these structures:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "G7KRa7DvGn6t"
      },
      "outputs": [],
      "source": [
        "for inputs, labels in train_dataloader:\n",
        "  print(f\"Input tensors are of shape: {inputs.shape}\")\n",
        "  print(f\"Labels for this batch are: {labels}\")\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kel2zZPDHO7K"
      },
      "source": [
        "Inputs in Pytorch are under the form of `torch.Tensor`. They are matrices of several dimensions. When looking at the shape of a tensor, like `(8, 1, 28, 28)`, the first dimension indicates the number of samples (or images here) that are contained in the tensor, and the other dimensions represent the content of the object (here the pixel values).\n",
        "\n",
        "Here, our tensor is of shape `(8, 1, 28, 28)`. The first dimension, of size `8`, indicates that the tensor contains 8 images, which is the _batch size_ (defined in the previous cell). The images are of shape `(1, 28, 28)`, which means 28 pixels x 28 pixels and 1 color channel.\n",
        "\n",
        "Accordingly, the labels tensor is of size `8`: the label of an image is a single number, and we consider 8 images in this batch, hence the size 8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZin_kKGH8ho"
      },
      "source": [
        "### Common datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvc05egWH_TK"
      },
      "source": [
        "Here, we have generated a dataset and a dataloader from personal images that were stored in one of our directories. As you have seen, this can be quite tedious.\n",
        "\n",
        "However, numerous public image datasets are already contained in Pytorch and can be loaded very easily. For instance, the image of clothes we are working on are extracted from a dataset called [_\"Fashion MNIST\"_](https://github.com/zalandoresearch/fashion-mnist), which we can load with the following lines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wslC5MgJI6aq"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "downloaded_path = \"./downloaded_datasets\"\n",
        "train_dataset = FashionMNIST(downloaded_path, train=True, download=True)\n",
        "test_dataset = FashionMNIST(downloaded_path, train=False, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boNWUEMZJxvp"
      },
      "outputs": [],
      "source": [
        "idx = 42\n",
        "image, label = train_dataset[idx]\n",
        "\n",
        "plt.imshow(image, cmap=\"gray\")\n",
        "plt.title(f\"Label: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR4d1IV4JDLp"
      },
      "source": [
        "You can find a list of some of these datasets [here](https://pytorch.org/vision/stable/datasets.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLPXpKmks1H6"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfv_B1W3Ny0V"
      },
      "source": [
        "### Define a sequence of transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSFWVVopNih8"
      },
      "outputs": [],
      "source": [
        "# Utils function for following cells\n",
        "def compare_transforms(transforms):\n",
        "  base_train_dataset = FashionMNIST(downloaded_path, train=True, download=True)\n",
        "  transformed_train_dataset = FashionMNIST(downloaded_path, train=True, transform=transforms, download=True)\n",
        "\n",
        "  idx = 42\n",
        "\n",
        "  plt.subplot(121)\n",
        "  img, lbl = base_train_dataset[idx]\n",
        "  plt.imshow(img, cmap=\"gray\")\n",
        "  plt.title(f\"Base image (label: {lbl})\")\n",
        "\n",
        "  plt.subplot(122)\n",
        "  img, lbl = transformed_train_dataset[idx]\n",
        "  plt.imshow(img, cmap=\"gray\")\n",
        "  plt.title(f\"Transformed image (label: {lbl})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k6bM_8eKvzJ"
      },
      "source": [
        "Training neural networks on images requires to present a very large diversity of images to a model. In practice, we often have limited amount of data available. To artificially increase these datasets, we can resort to what is called _data augmentation_.\n",
        "\n",
        "The principle is simple: we randomly apply some transformations on our images to simulate a different point of view, a different lightning, object, camera parameters, etc. Torchvision, a library used jointly with Pytorch, gives us tools very easy to apply:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HUZIyv5s04C"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define transforms to apply and visualize the results\n",
        "transforms = torch.nn.Sequential(\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomRotation(35),\n",
        "    transforms.CenterCrop(20),\n",
        ")\n",
        "\n",
        "compare_transforms(transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAgpQ_nrLfQM"
      },
      "source": [
        "You may run several times the previous cell and obtain different results every time.\n",
        "\n",
        "You can find a list of the available augmentations [here](https://pytorch.org/vision/stable/transforms.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN9tiapDN1cF"
      },
      "source": [
        "### Evaluate the impact of the augmentation on the performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRJf_-h2N6UA"
      },
      "source": [
        "We evaluate in the next cell how much the previous data augmentation improves the accuracy of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05na9rdBN5O8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from casablanca_course.data import train_for_transforms\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "dataset = \"fashionmnist\"\n",
        "train_share = 15\n",
        "\n",
        "def train(transformations):\n",
        "  train_for_transforms(transformations, dataset=dataset, batch_size=32, n_epochs=6, train_share=train_share, show_images=True)\n",
        "\n",
        "# Without augmentations\n",
        "print(\"=\" * 30, \"\\nWITHOUT AUGMENTATIONS\\n\")\n",
        "no_transforms = transforms.ToTensor()\n",
        "train(no_transforms)\n",
        "\n",
        "# With some augmentations\n",
        "print(\"=\" * 30, \"\\nWITH AUGMENTATIONS\\n\")\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.ColorJitter(brightness = 0.15, contrast = 0.2, saturation = 0.2),\n",
        "])\n",
        "train(train_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9btHog7Vs6Dp"
      },
      "source": [
        "## Testing dataset size and data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP3NEiQSqWsO"
      },
      "source": [
        "### Dataset size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VohOClIKWfT"
      },
      "source": [
        "Neural networks are very intensive in terms of data they require to be trained on.\n",
        "\n",
        "In the next cell, we train a model on the CIFAR10 dataset, a dataset of small images of 10 categories including cars, planes or horses among others. We can adjust the share of the training set on which we train the model through the variable `train_share`. Run the cell several times with various train shares and have a look at how the validation loss evolves depending on the number of samples fed to the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUK5hRhatHdi"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from casablanca_course.data import train_for_transforms\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Change the share below (the value x means x% of the train set)\n",
        "train_share = 100 # %\n",
        "\n",
        "train_for_transforms(train_transforms, batch_size=32, n_epochs=2, train_share=train_share, show_images=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0gQEtoPKYPB"
      },
      "source": [
        "### Data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3fwawDurQcH"
      },
      "source": [
        "As we explained previously, the data augmentation impacts the performance of the trained model. This is visible with the validation loss.\n",
        "\n",
        "In the next cell, change the transformations of `train_transforms` to increase the diversity of images in the training set and check the impact on the validation loss.\n",
        "\n",
        "You can find other usable augmentations [here](https://pytorch.org/vision/stable/transforms.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LE7kIll1r7ub"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_for_transforms(train_transforms, batch_size=32, n_epochs=4, train_share=100, show_images=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbsDY_Vx-1m8"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUvHGZ-a-3FP"
      },
      "source": [
        "- [Pytorch documentation for Dataset and DataLoader](https://pytorch.org/docs/stable/data.html)\n",
        "- [Common datasets in Pytorch](https://pytorch.org/vision/stable/datasets.html)\n",
        "- [Documentation for data augmentation](https://pytorch.org/vision/main/transforms.html)\n",
        "- [Many public datasets used in research](https://paperswithcode.com/datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXVt7-hAk9yW"
      },
      "source": [
        "_Practical session written by Thomas Chabal and ClÃ©ment Riu â¢ Spring School on Data Science - Ecole Centrale Casablanca â¢ 2022._"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
